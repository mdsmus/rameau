
	NILE V0.2b USER GUIDE

As the code is set up to be run from shell scripts if not from within
a LISP session, there are no command line options to be
documented. This document will give an introduction on which options
are available for the main functions which are used by the shell
scripts documented in "install_guide" and "demo_guide", as well as an
overview of the general structure of the implementation, describing in
more detail some of the fundamental functions/macros.



Compile-time parameters
=======================

Many parameters of the simulations have to be set at compile/macro
expansion time, which is accomplished by definitions of parameters or
constants at the beginning of each source code file. The most
important parameters are:

nile_batali
-----------

(defparameter *numchars* 4)

The number of characters to be used for the simulation. Defaults to 4,
should be at least 2. Though the maximum is (- most-positive-fixnum
*hidden-size*), it should be kept below 26 in order to allow the
characters to be represented as those of the latin alphabet.

(defconstant *popsize* 30)

The size of the population

(defparameter *hidden-size* 30)

The size of the agent's hidden layer. Has to be (<= (+ *hidden-size*
*numchars*) most-positive-fixnum)

(defconstant *max-word-length* 20)

The cut off-length

nile_gen
--------

Same as for nile_batali

nile_learnability
-----------------

Same as for nile_batali


nile_demos
----------

The size of the networks to be used is defined by (netspec ....) at
various places within the code, e.g. (netspec 5 5 1) for the 5-5-1
network used in demo-par5.


The top-level functions
=======================

nile_batali:run-simulation
--------------------------

Options which can be specified at run-time are

:max-cycles fixnum

	maximum number of rounds

:write-pop-every fixnum

	after how many rounds to dump the population

:num-teachers fixnum

	number of teachers in every communication episode

:num-meanings fixnum

	number of meanings (out of 100, randomly chosen) to be used
	for generation of surface forms by the teachers as every
	communication episode

:method 'nile_batali::sd or 'nile_batali::cg

	for sd, the function nile_batali:batali-round-sd is used to
	run a round, for cg, the function
	nile_batali:batali-round-cg. The latter was not used for the
	thesis

:eta double-float

	Learning rate for sd updates

:post-round-func function

	A function to be applied as follows after each round:

	(funcall post-round-func population cycle)

	Can be used for example to dump the words generated at each
	round to a stream:

	(defun pop-writer (pop round)
	  (nile_stats::tabular-text (nile_batali::get-words pop)
	                            :stream *pop-stream*
	                            :name (format nil "Round ~a" round)))
	(nile_batali:run-simulation     :max-cycles 25000
	                                :write-pop-every 40000
	                                :num-meanings 100
	                                :eta 0.01d0
	                                :post-round-func #'pop-writer)

:random-state random-state

	A random state which to use to initialise the networks'
	weights.

nile_gen:run-simulation
-----------------------

Same as nile_batali:run-simulation, except that :num-teachers
specifies the number of training cycles between generations, as there
is only one teacher (the agent from each previous population).

nile_learnability:train-lang
----------------------------

This function is just a wrapper to various other (possibly
commented-out) functions of the same source file. It does hardly more
than passing on the arguments to these functions.

The first argument has to be a language training set as generated by
nile_learnability:ideo-words-fun. (See source of
nile_learnability:runit for an example)

Some other arguments are:

:method

	selects the training function to use. See source.
	For replication of runs of the thesis, should be
	:batali

:eta double-float

	Learning rate for sd-updates

:alpha double-float

	Momentum for sd-updates

:random-state random-state

	A random-state to initialise the random number generator with
	before initialising the network's weights

:passes fixnum

	Number of passes through the training set

:max-cycles fixnum

	Maximum number of training cycles of each pass

:error-lim double-float

	Error limit for each pass

:write-error-every fixnum

	Number of cycles after which to write the error to
	*standard-output*

:write-net-every fixnum

	Number of cycles after which to write the network to
	*standard-output*

:write-final fixnum

	Whether to write the error after all cycles of a
	pass


For more details of the arguments used here, see the following
documentation of details of the implementation.


Details of the implementation
=============================

Many of the options discussed (and not discussed) above are just being
passed on to a couple of functions central to the ANN simulation
mechanisms implemented in this software.

Almost all of this functionality is part of the nile_compile-net-lib
package. The reader is also referred to the source code of this
package. All macros which build code to run ANNs are using a
compile-time network-specification defined as the structure
nile_c2ompile-net-lib:net-spec. Its element layer-specs is a list of
nile_c2ompile-net-lib:layer-spec structures, specifying the layers of
the net, from input- to output-layer.

For Elman- and other simple recurrent networks, a list of pairs of
nodes can be specified for which to copy activation values. See
documentation in the source of the definition of
nile_c2ompile-net-lib:net-spec.

The actual net is is defined as a vector of structures defined in
nile_run-net-lib:layer, the first element being the input layer and
the last the output layer. The layer structure not only holds weights
and activation values, but also derivatives calculated by back
propagation, possibly a history of updates done etc.


The following macros are central to ANN simulations (all from
nile_compile-net-lib)


(defmacro activation-fn (netspec-form net
			 &key (copy-recurrent nil)
			      (activate t)
			      ;; whether to return statistics
			      ;; of context units
			      return-context-unit-stats)

Activates the net passed as argument net, which has to be of the type
specified in netspec-form. The macro builds code which in order to
avoid code duplication should be embedded in a function definition.

Activation is done if :activate is true; if :copy-recurrent is true,
the recurrent connections specified in netspec-form are also
copied. return-context-unit-stats is only used for experimental
purposes. All parameters except for net have to be given at macro
expansion time.

(defmacro adjust-weights (netspec-form	; the compile-time net-specification
			  net		; the net-array of layer-structures
			  &key eta	; learning rate for update w <- w - eta * de/dw
			       ;; momentum:
			       ;; update to w <- w - eta * de/dw + mom * prev_update
			       ;; Implies save-update
			       momentum
			       ;; save-weights:
			       ;; either 'copy -> copies to saved-weight-vec
			       ;; or     'set  -> sets saved-weight-vec to weight-vec and
			       ;;                 sets weight-vec to new array
			       save-weights
			       ;; use the saved weights t/nil
			       use-saved-weights
			       ;; clear de-by-dws t/nil
			       clear-de-by-dws
			       ;; save the update made into previous-update-vec
			       save-update
			       ;;
			       cg-update
			       ;; see above
			       make-cg-updates-with-beta
			       ;; kick-weights
			       kick-weights)

Updates the weight-vectors of a net. eta the leaning rate by which to
update the weights with the corresponding de-by-dw value. If
save-weights is either :copy or :set, before the update, the weights
are saved to the corresponding entry of the saved-weight-vec element
of the layer structure by either setting the saved-weight-vec to the
old weight-vec (:set) or copying each element (:copy). The former is
faster, but produces more garbage. All parameters except for net, eta
and momentum have to be given at macro expansion time.

If use-saved-weights is true, the saved weights are used for the
update. If save-update is true, the weight-update is saved to the
previous-update-vec element of each layer.

cg-update has to be set to t, if conjugate gradient updates are to be
done in order to set saved weights appropriately. The beta value for
cg updates can be specified by make-cg-updates-with beta. kick-weights
is still experimental.


(defmacro backprop-fn (netspec-form net dest
		       &key scale
			    scale-just-elman-recurrents
			    improved-elman
			    improved-elman-opts)

Calculates the dE-by-Dw values for the net. All keyword options are
experimental. All parameters except for net, dest and scale have to be
given at macro expansion time.


(defmacro beta (layer-specs net &key (variant :polak-ribiere)
				     zero-beta-save-derivs)

Builds a function to calculate the beta value used for conjugate
gradient updates. The variant can be specified as :polak-ribiere,
:fletcher-reeves or :hestenes-steifel.

If zero-beta-save-derivs is true, a function is built which returns
zero, and saves the current de-by-dws to the previous-de-by-dw vector.
All parameters except for net have to be given at macro expansion
time.


Cycles, steps, passes and the versatile trainer macro
-----------------------------------------------------

As the code can both handle normal feed forward and recurrent
networks, the number of training methods easily becomes
confusing. First of all, training patterns have to be set up as
follows:

- For normal feed-forward nets, a vector of input layers (as
  nile_run-net-lib:layer structures) forms the input pattern set.

- Consequently, the destination pattern set is a vector of the same
  size, but of output-layers.

The elements of both vectors, layers, have to be of the same size as
the input- and output-layer of the net, respectively. Please consider
that, to activate a net during training, it's input layer is *set* to
the instance of the input pattern training set, to avoid copying.


For recurrent nets, the code currently assumes the destination
patterns to be equal for all time steps. But as input patterns usually
vary over time, for recurrent nets, they are a set of patterns for
each time step, each set of patterns being one as for feed forward
nets.

For Elman recurrent nets (other types of recurrent networks are
currently not completely supported), the input layer is assumed to
consist of the real input units, followed by the context units. To
train time series on Elman nets with some of the training methods
implemented, the hidden unit activations after activation of the
patterns of one time step are copied to the context units of the
pattern set of the next time step.


Besides the two update methods (steepest descent with momentum and
conjugate gradients with line searches), the code allows batches over
time series and patterns.

CYCLES are the iterative steps of consecutive updates of the training
       method 

STEPS are the iterative steps over time steps

PASSES are the iterative steps over the whole training set

E.g. for time-batch, pattern-batch, the error derivatives are summed
up over patterns (cycles) and time-steps, before an update is done.

	passes
	  -> CYCLES (update here)
	    -> steps
	        -> patterns

For pattern-iterative time-batch, the error derivatives are summed up
over all time steps of a pattern, then an update is done, before the
next pattern is trained.
	
	passes
	  -> patterns
	      -> CYCLES (update here)
	          -> steps


All of this functionality is implemented in one macro,
nile_compile-net-lib:trainer.

(defmacro trainer (netspec-form net
		   ;; the input patterns:
		   ;; if (not elman-net)
		   ;;     vector of <numpatterns> input-layers
		   ;; else
		   ;;     vector of <num-steps>
		   ;;      vectors of <numpatterns> input-layers
		   input-patterns
		   ;; vector of <numpatterns> output-layers
		   dest-patterns
		   &key numpatterns	; number of patterns (of each time step)
			elman-net	; t if elman net
			num-steps	; number of time steps to be trained
			improved-elman	; EXPERIMENTAL
			;;
			(batch t)	; if to do batch over patterns
			timebatch	; if to do batch over timesteps
			;;
			(method :sd)	; :sd or :cg
			sd-opts		; options to pass to SD trainer
			cg-opts		; options to pass to CG trainer
			;;
			max-cycles	; maximum number of cycles
			;; and an error limit at which to stop
			(error-lim (coerce 0.01d0 'type-act)) 
			;;
			;; opts for cycles (writers)
			cycle-opts
			;; opts for steps (writers)
			step-opts
			;;
			;; whether to write both the final error and the final
			;; net to *standard-output* (compatibility arg)
			write-final  
			write-final-error ; whether to write the final error
			write-final-net	; whether to write the final net
			return-final-error ; whether to return the final error
			;;
			;; Local minima detection is EXPERIMENTAL
			;; if to do proprietary local minimum detection
			proprietary-detect-locmin
			;; amout that to kick when local minimum detected
			(proprietary-detect-locmin-kick (coerce 2.0d0 'type-weight))
			;; factor to calculate locmin-measure
			(proprietary-detect-locmin-x-error-factor (coerce 0.0001d0
								   'type-act))
			;; external-functions
			ext-funcs)

The SD-options are the following:

:eta		the learning rate
:momentum	the momentum

The CG-options are

:save-weights-method (:SET or :COPY)

	Whether to save weights by setting the previous-weight-vec
	elements of the layer structures to the current weight-vector,
	or by copying the vector's elements.

:interval-location-maximum type-weight

	The maximum update step to do.
	This is because the interval location would sometimes find
	an infinite update step.

:interval-location-eta

	eta value for interval location of the CG-linesearch
	procedure. Basically, this determines the size of steps of
	updates to try to find a local minimum in the space of the
	error over the weights along the line which is conjugate to
	all previous updates and the current direction of steepest
	descent on the error surface.

:gss-tol

	A tolerance value to use for golden section search (also part
	of CG-linesearch procedure)

:cg-opt-max-cg-updates

	Maximum number of consecutive CG-updates before doing a
	steepest-descent step.


N.B.: In the nile_demos package, all demos except for demo-par5 are
still using two different macros, train-sd and train-cg for the two
different update methods. The funcitonality of both has meanwhile been
incorporated into that of the single (and unfortunately quite massive)
trainer macro. For a future release, the code of nile_demos will be
adapted to the new macro.

