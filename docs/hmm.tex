\documentclass{article}
\usepackage{amsmath}
\usepackage[utf8x]{inputenc}

\title{Algoritmo de Hidden Markov Model para o Rameau}
\author{Alexandre Tachard Passos}

\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg\,max}}\,}

\begin{document}
\maketitle

\section{Introdução}
\label{sec:intro}

Um HMM (Hidden Markov Model) é uma extensão das cadeias de Markov
(Markov Chains, modelos estatísticos para predição de sequências) para
ambientes onde não se tem toda a informação necessária pra saber os
estados da sequência.
                                                                   
O objetivo de se usar um HMM para análise harmônica é prever a melhor
análise possível para uma dada sequência de notas. Para isso, um HMM é
dividido em duas partes. Em primeiro lugar se tem uma cadeia de Markov
normal, representando os acordes ou os graus da análise funcional. A
partir daí se tira a probabilidade de um acorde, ou de uma cadência
como ii V I. Essa é a parte oculta do modelo. Ela se chama oculta por
não ser diretamente observável na música---se ouvem notas, não
acordes. Para completar o modelo se associa uma distribuição de
probabilidade de cada ``acorde'' desses gerar cada conjunto de notas
possível. A análise harmônica, então, é a sequência de acordes que
maximiza a probabilidade da sequência de notas observada, ou $
\argmax{h} \, P(h|N) $, onde $h$ é a análise
harmônica, $N$ é a sequência de notas e $P(h|N)$ é a probabilidade da
análise harmônica dado a sequência de notas.

\section{Simplificações}
\label{sec:simpl}

Para extrair essa probabilidade, algumas assumições são feitas:    
\begin{itemize}                                                    
 \item As notas observadas em um dado instante dependem apenas do  
   acorde ``escondido'' naquele instante                           
 \item O acorde ``escondido'' em um dado instante é unicamente     
   determinado pelo acorde escondido imediatamente precedente.     
\end{itemize}                                                      
                                                                   
Assim, pelo teorema de Bayes,
\[P(h|N) = \frac{P(N|h)P(h)}{P(N)}.\]

Como $P(N)$ é constante em todas as análises, podemos retirar da
fórmula. Expandindo, então, temos que a análise correta $h_1 \ldots
h_n$ é
\[\argmax{h_1 \ldots h_n} \prod_{i=1}^n P(N_i|h_i)P(h_i|h_{i-1}),\]
onde as probabilidades $P(N_i|h_i)$ e $P(h_i|h_{i-1})$ são
determinadas a partir dos dados de treinamento.

Para determinar $P(N_i|h_i)$ basta construir um modelo que associe
acordes a conjuntos de notas baseado no corpus de treinamento. Para
determinar $P(h_i|h_{i-1})$ basta determinar uma matriz de transição
de acordes no corpus de treinamento. Essa matriz é também chamada de
um modelo bi-gram de acordes.

Nessa implementação faremos ainda outra assumição simplificadora, a de
que, dado um acorde $h_i$, as notas que o formam são aproximadamente
independentes entre si, ou seja, para $N_i = \{n_1, \ldots , n_m\}$,
$P(N_i|h_i) = \prod_{j=1}^m P(n_j|h_i)$.

\section{Algoritmo de Viterbi}
\label{sec:viterbi}

Por causa da propriedade de independência e por querermos encontrar
apenas o evento de maior probabilidade a análise pode ser feita em
$O(n*m)$, onde $n$ é o número de sonoridades e $m$ é o número de
acordes possíveis. Para isso, constrói-se duas tabela $T[i,j]$ e
$M[i,j]$ que associam o acorde $j$ à sonoridade $i$, e se preenche
assim:
\begin{itemize}
\item Para a primeira linha, $T[0,j] = P(j|N_0)$, onde $N_0$ é
  o conjunto de notas da sonoridade $0$. $M[0,j] = nil$.
\item Para cada linha consecutiva, 
  \begin{eqnarray}
 T[i,j] &=& \operatorname{max} \{ P(N_i|j)P(j|j')T[i-1,j'] \forall j' \in h \} \\
 M[i,j] &=& \argmax{j'} \{ P(N_i|j)P(j|j')T[i-1,j'] \}
\label{eq:1}
\end{eqnarray}
onde $h$ é o conjunto de acordes possíveis.
\end{itemize}
A análise ideal, então, é o melhor caminho nessa tabela, calculado com
Backtracking. Esse é o algoritmo de Viterbi.

Para simplificar as contas, o algoritmo usa, sempre que possível,
$\log P(x)$ em vez de $P(x)$, já que multiplicar os muitos números
pequenos pode causar underflow. Assim, as fórmulas ficam
\begin{eqnarray}
T[0,j] &=& \log P(j|N_0) \\
T[i,j] &=& \operatorname{max} \{ \log P(N_i|j) + \log P(j|j') + T[i-1,j'] \} \\
M[i,j] &=& \argmax{j'} \{ \log P(N_i|j) + \log P(j|j') + T[i-1,j']\}.
\label{eq:2}
\end{eqnarray}

\section{Indução do modelo}
\label{sec:modelo}

Como foi visto nas seções acima, para fazer a análise harmônica é
necessário precomputar três funções de probabilidade: $P(N_i|h_i)$,
que representa a probabilidade de obter as notas de um segmento dado a
análise do segmento; $P(h_i|h_{i-1})$, que representa a probabilidade
de um acore $h_{i-1}$ ser seguido por um acorde $h_i$; e $P(h_i)$,
usada para inicializar o algoritmo.

A princípio, todas essas probabilidades poderiam ser deduzidas apenas
a partir de uma simples contagem de quantas vezes cada evento desses
ocorre no corpus de treinamento. Isso, no entanto, apresenta alguns
problemas, concentrando massa probabilística demais nos eventos
observados e ignorando os não-observados. Para evitar esse problema
várias técnicas de \textit{smoothing} podem ser adotadas. A mais
simples, que se chama \textit{``add-one smoothing''}, consiste em
simplesmente adicionar $1$ a todas as contagens observadas. Isso, no
entanto, tem o problema oposto, concentrando massa probabilística
demais nos eventos não-observados. A ideal de se utilizar se chama
\textit{``Good-Turing smoothing''}, mas é complicada demais e não foi
implementada ainda. O smoothing adotado atualmente consiste em usar
uma função de probabilidade anterior (\textit{prior probability}) que
é um processo de Dirichlett. Assim, a probabilidade de cada símbolo
$s$ observado fica $F_s/(\alpha + F)$, onde $\alpha$ é um parâmetro e
$F_s$ é a frequência em que $s$ aparece e $F$ é o total de
observações; e a frequência de cada símbolo não-observado fica
$\alpha/(\alpha + F)$. Isso apresenta resultados razoáveis, como pode
ser conferido nas visualizações das frequências.

\end{document}
